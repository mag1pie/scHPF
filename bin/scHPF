#! /usr/bin/env python

import os
import sys
import argparse
import ast

import json
import time
from functools import partial
import yaml
from copy import deepcopy

import time

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

import numpy as np
import pandas as pd
from scipy.io import mmread, mmwrite
from scipy.sparse import coo_matrix
from scipy.stats import sem
import joblib
import glob
from schpf import scHPF, run_trials, run_trials_pool
from schpf.util import max_pairwise_table, mean_cellscore_fraction_list, get_param_dfs, get_spectra, \
get_genescore_spectra, get_spectra_order, get_local_density, make_consensus_plot, \
refit_local_params, get_ranked_genes, model_selection, clustering
from schpf.preprocessing import load_coo, load_and_filter, load_like
from schpf.preprocessing import split_validation_cells
from schpf.loss import projection_loss_function, mean_negative_pois_llh

import umap
import matplotlib as mpl
from matplotlib import pyplot as plt
plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] =42
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import gridspec

import plotly.express as px
import plotly.io as pio

import igraph as ig

import scipy.sparse as sp
from scipy.spatial.distance import squareform
#from sklearn.decomposition.nmf import non_negative_factorization
#from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn.metrics import silhouette_score
from sklearn.utils import sparsefuncs
import sklearn

def _parser():
    # usage = """scHPF <command> [<args>]

# The most commonly used scHPF commands are:
    # prep    Prepare data
    # train   Train a model from data
    # score   Get cell-scores, gene-scores, and other data

# Some advanced scHPF commands are:
    # prep-like    Prepare data with the same genes & order as other data
    # project      Project data onto a pre-trained model
    # train-pool   Train a model, parallelized at the level of trials
    # """
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='cmd')

    ### Preprocess command
    prep = subparsers.add_parser('prep',
            help='Prepare data for training')
    # data
    prep.add_argument('-i', '--input', required=True,
            help='Input data. Currently accepts either: (1) a whitespace-'
            'delimited gene by cell UMI count matrix with 2 leading columns '
            'of gene attributes (ENSEMBL_ID and GENE_NAME respectively), or '
            '(2) a loom file with at least one of the row attributes '
            '`Accession` or `Gene`, where `Accession` is an ENSEMBL id and '
            '`Gene` is the name.'
            )
    prep.add_argument('-o', '--outdir', 
            help='Output directory. Does not need to exist.')
    prep.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # gene filtering criteria
    prep.add_argument('-m', '--min-cells', type=float, default=0.01, 
            help='Minimum number of cells in which we must observe at '
            'least one transcript of a gene for the gene to pass '
            'filtering. If 0 <`min_cells`< 1, sets threshold to be '
            '`min_cells` * ncells, rounded to the nearest integer.'
            ' [Default 0.01]')
    prep.add_argument('-w', '--whitelist', default='',
            help='Tab-delimited file where first column contains ENSEMBL gene '
            'ids to accept, and second column contains corresponding gene '
            'names. If given, genes not on the whitelist are filtered from '
            'the input matrix. Superseded by blacklist. Optional.')
    prep.add_argument('-b', '--blacklist', default='',
            help='Tab-delimited file where first column contains ENSEMBL gene '
            'ids to exclude, and second column is the corresponding gene name. '
            'Only performed if file given. Genes on the blacklist are '
            'excluded even if they are also on the whitelist. Optional.')

    # optional selection of cells for validation set
    prep.add_argument('-nvc', '--n-validation-cells', type=int, default=0,
            help='Number of cells to randomly select for validation.')
    prep.add_argument('-vgid', '--validation-group-ids', default=None,
            help= 'Single column file of cell group ids readable with '
            ' np.readtxt. If `--n-validation-cells` is > 0, cells will be '
            ' randomly selected approximately evenly across the groups in '
            ' this file, under the constraint that at most'
            ' `--validation-min-group-frac` * (ncells in group) are selected'
            ' from every group.')
    prep.add_argument('--validation-max-group-frac', type=float, default=0.5,
            help='If `-nvc`>0 and `validation-group-ids` is a valid file, at'
            ' most `validation-min-group-frac`*(ncells in group) cells are'
            ' selected from each group.')

    # other options
    prep.add_argument('--filter-by-gene-name', default=False, 
            action='store_true', help='Use gene name rather than ENSEMBL'
            ' id to filter (with whitelist or blacklist).  Useful for'
            ' datasets where only gene symbols are given. Applies to both'
            ' whitelist and blacklist. Used by default when input is a loom'
            ' file (unless there is an Accession attribute in the loom).')
    prep.add_argument('--no-split-on-dot', default=False, action='store_true',
            help='Don\'t split gene symbol or name on period before '
            'filtering whitelist and blacklist. We do this by default for '
            'ENSEMBL ids.')


    #### Prepare like
    prep_like = subparsers.add_parser('prep-like', 
            help='Prepare a data set like another (ie with the same genes in'
            ' the same order)')
    # data
    prep_like.add_argument('-i', '--input', required=True,
            help='Input data to format. Currently accepts either: (1) a'
            ' whitespace-delimited gene by cell UMI count matrix with 2'
            ' leading columns of gene attributes (ENSEMBL_ID and GENE_NAME'
            ' respectively), or (2) a loom file with at least one of the row'
            ' attributes `Accession` or `Gene`, where `Accession` is an'
            ' ENSEMBL id and `Gene` is the name.')
    prep_like.add_argument('-r', '--reference', required=True,
            help='Two-column tab-delimited file of ENSEMBL ids and gene names'
            ' to select from `input` and order like. All genes in `reference`'
            ' must be present in `input`.')
    prep_like.add_argument('-o', '--outdir', required=True,
            help='Output directory. Does not need to exist.')
    prep_like.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')
    # other options
    prep_like.add_argument('--by-gene-name', default=False, 
            action='store_true', help='Use gene name rather than ENSEMBL'
            ' id to when matching against reference.  Useful for datasets'
            ' where only gene symbols are given. Used by default when input'
            ' is a loom file (unless there is an Accession attr in the loom).')
    prep_like.add_argument('--no-split-on-dot', default=False, action='store_true',
            help='Don\'t split gene symbol or name on period before'
            ' when matching to reference. We do this by default for ENSEMBL'
            ' ids.')


    ###### Train command
    train = subparsers.add_parser('train',
            help='Train a model with automatic parallelization across'
            ' computations with numba')
    # data and saving
    train.add_argument('-i', '--input', required=True,
            help="Training data. Expects either the mtx file output by the "
            "prep command or a tab-separated tsv file formatted like:" 
            "`CELL_ID\tGENE_ID\tUMI_COUNT`. In the later case, ids are "
            "assumed to be 0 indexed and we assume no duplicates."
            )
    train.add_argument('-o', '--outdir', 
            help='Output directory for scHPF model. Will be created if does '
            'not exist.')
    train.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # Required model hyperparameter
    train.add_argument('-k', '--nfactors', type=int, required=True,
            help='Number of factors.')

    # training parameters
    train.add_argument('-t', '--ntrials',  type=int, default=1,
            help='Number of times to run scHPF, selecting the trial with '
            'best loss (on training data unless validation is given).'
            ' [Default 1]')
    train.add_argument('-v', '--validation-cells', default=None,
            help='Cells to use to assess convergence and choose a model.'
            ' Expects same format as ``-i/--input``. Training data used by'
            ' default.'
            )
    train.add_argument('-M', '--max-iter', type=int, default=1000,
            help='Maximum iterations. [Default 1000].')
    train.add_argument('-m', '--min-iter', type=int, default=30,
            help='Minimum iterations. [Default 30]')
    train.add_argument('-e',  '--epsilon', type=float, default=0.001,
            help='Minimum percent decrease in loss between checks to continue '
            'inference (convergence criteria). [Default 0.001].')
    train.add_argument('-f', '--check-freq', type=int, default=10,
            help='Number of iterations to run between convergence checks. '
            '[Default 10].')
    train.add_argument('--better-than-n-ago', default=5, type=int,
            help= 'Stop condition if loss is getting worse.  Stops training '
            'if loss is worse than `better_than_n_ago`*`check-freq` training '
            'steps ago and getting worse. Normally not necessary to change.')
    train.add_argument('-a', type=float, default=0.3,
            help='Value for hyperparameter a. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    train.add_argument('-c', type=float, default=0.3,
            help='Value for hyperparameter c. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    train.add_argument('--float32', action='store_true',
            help="Use 32-bit floats instead of default 64-bit floats in"
            " variational distrubtions")
    train.add_argument('-bs', '--batchsize', default=0, type=int,
            help="Number of cells to use per training round. All cells used if"
            " 0. Note that using batches changes the order of updates during"
            " inference.")
    train.add_argument('-sl', '--smooth-loss', default=1, type=int,
            help="Average loss over the last `--smooth-loss` interations."
            " Intended for when using minibatches, where int(ncells/batchsize)"
            " is a reasonable value"
            )
    train.add_argument('-bts', '--beta-theta-simultaneous', action='store_true',
            help="If False (default), compute beta update, then compute theta"
            " based on the updated beta. Note that if batching is used, this"
            " order is reverse. If True, update both beta and theta based on"
            " values from the last training round. The later slows the rate of"
            " convergence and sometimes results in better log-likelihoods, but"
            " may increase convergence time, especially for large numbers of"
            " cells."
            )
    train.add_argument('-sa', '--save-all', action='store_true',
            help="Save all trials")
    train.add_argument('-rp', '--reproject', action='store_true',
            help="Reproject data onto fixed global (gene) parameters after"
            " convergence, but before model selection. Recommended with"
            " batching")
    train.add_argument('--quiet', dest='verbose', action='store_false', 
            default=True, help="Don't print intermediate llh.")

    ###### train with trials in threadpool
    train_pool = subparsers.add_parser('train-pool', parents=[train], 
            add_help=False, conflict_handler='resolve')
    train_pool.add_argument('--njobs', type=int, default=0,
            help='Max number of processes to spawn. 0 will use the minimum of'
            ' all available cores and ntrials.')
    # Required model hyperparameter
    train_pool.add_argument('-k', '--nfactors', nargs='+', type=int, 
        required=True, help='Number of factors.')

    ###### Multiple train command
    multi_train = subparsers.add_parser('multi-train',
            help='[MC] automated training process through a series of K values')
    # data and saving
    multi_train.add_argument('-i', '--input', required=True,
            help="Training data. Expects either the mtx file output by the "
            "prep command or a tab-separated tsv file formatted like:" 
            "`CELL_ID\tGENE_ID\tUMI_COUNT`. In the later case, ids are "
            "assumed to be 0 indexed and we assume no duplicates."
            )
    multi_train.add_argument('-o', '--outdir', 
            help='Output directory for scHPF model. Will be created if does '
            'not exist.')
    multi_train.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # Required model hyperparameter
    multi_train.add_argument('--minK', type=int, required=True,
            help='Minimal number of factors.')
    
    multi_train.add_argument('--maxK', type=int, required=True,
        help='Maximal number of factors.')
    
    multi_train.add_argument('-nm', '--nmodels', type=int, required = True,
            help='Number of Models to run per K.')    
    multi_train.add_argument('--njobs', type=int, default=0,
            help='Max number of processes to spawn. 0 will use the minimum of'
            ' all available cores and ntrials.')
        
    # training parameters
    multi_train.add_argument('-t', '--ntrials',  type=int, default=1,
            help='Number of times to run scHPF, selecting the trial with '
            'best loss (on training data unless validation is given).'
            ' [Default 1]')
    multi_train.add_argument('-v', '--validation-cells', default=None,
            help='Cells to use to assess convergence and choose a model.'
            ' Expects same format as ``-i/--input``. Training data used by'
            ' default.'
            )
    multi_train.add_argument('-M', '--max-iter', type=int, default=1000,
            help='Maximum iterations. [Default 1000].')
    multi_train.add_argument('-m', '--min-iter', type=int, default=30,
            help='Minimum iterations. [Default 30]')
    multi_train.add_argument('-e',  '--epsilon', type=float, default=0.001,
            help='Minimum percent decrease in loss between checks to continue '
            'inference (convergence criteria). [Default 0.001].')
    multi_train.add_argument('-f', '--check-freq', type=int, default=10,
            help='Number of iterations to run between convergence checks. '
            '[Default 10].')
    multi_train.add_argument('--better-than-n-ago', default=5, type=int,
            help= 'Stop condition if loss is getting worse.  Stops training '
            'if loss is worse than `better_than_n_ago`*`check-freq` training '
            'steps ago and getting worse. Normally not necessary to change.')
    multi_train.add_argument('-a', type=float, default=0.3,
            help='Value for hyperparameter a. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    multi_train.add_argument('-c', type=float, default=0.3,
            help='Value for hyperparameter c. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    multi_train.add_argument('--float32', action='store_true',
            help="Use 32-bit floats instead of default 64-bit floats in"
            " variational distrubtions")
    multi_train.add_argument('-bs', '--batchsize', default=0, type=int,
            help="Number of cells to use per training round. All cells used if"
            " 0. Note that using batches changes the order of updates during"
            " inference.")
    multi_train.add_argument('-sl', '--smooth-loss', default=1, type=int,
            help="Average loss over the last `--smooth-loss` interations."
            " Intended for when using minibatches, where int(ncells/batchsize)"
            " is a reasonable value"
            )
    multi_train.add_argument('-bts', '--beta-theta-simultaneous', action='store_true',
            help="If False (default), compute beta update, then compute theta"
            " based on the updated beta. Note that if batching is used, this"
            " order is reverse. If True, update both beta and theta based on"
            " values from the last training round. The later slows the rate of"
            " convergence and sometimes results in better log-likelihoods, but"
            " may increase convergence time, especially for large numbers of"
            " cells."
            )
    multi_train.add_argument('-sa', '--save-all', action='store_true',
            help="Save all trials")
    multi_train.add_argument('-rp', '--reproject', action='store_true',
            help="Reproject data onto fixed global (gene) parameters after"
            " convergence, but before model selection. Recommended with"
            " batching")
    multi_train.add_argument('--quiet', dest='verbose', action='store_false', 
            default=True, help="Don't print intermediate llh.")

    ###### train with trials in threadpool
    multi_train_pool = subparsers.add_parser('multi-train-pool', parents=[multi_train], 
            add_help=False, conflict_handler='resolve')
    multi_train_pool.add_argument('--njobs', type=int, default=0,
            help='Max number of processes to spawn. 0 will use the minimum of'
            ' all available cores and ntrials.')

    ###### Super consensus command
    consensus = subparsers.add_parser('consensus',
            help='Build a consensus model with an optimal number of factors from a series of models generated from multi-train')
    
    consensus.add_argument('-i', '--input', required=True,
            help="Training data. Expects the mtx file output by the "
            "prep command.")
    
    consensus.add_argument('-g', '--genefile', default=None,
            help='Create an additional file with gene names ranked by score '
            'for each factor. Expects the gene.txt file output by the scHPF '
            'prep command or a similarly formatted tab-delimited file without '
            'headers. Uses the zero-indexed ``--name_col``\'th column as gene '
            'names. Optional.')
    
    consensus.add_argument('-sfp', '--sample_folder_prefix', default=None,
        help="The prefix of sample paths; only required when running consensus for a single sample i.e. supercons = False [Default None]."
        )

    consensus.add_argument('-o', '--outdir', default=None,
    help="The path where a super consensus/ folder would be created; only required when supercons = True [Default None]."
    )
    consensus.add_argument('-p', '--prefix', default='',
                      help='Prefix for output files. Optional.')
    
    consensus.add_argument('-s', '--samples', type=str, default='',
            help='String of a list of unique sample IDs included; required when running sample-specific consensus model(s)')
    
    consensus.add_argument('-sl', '--sample_list_dir',default=None,
        help='Path to a list of sample IDs ordered by cells the input matrix [Default None]')

    consensus.add_argument('--nper', type=int, default=3,
            help='Top n models with minimal loss. [Default 3]')

    consensus.add_argument('--minK', type=int, required=True,
            help='Minimal number of factors.')
    
    consensus.add_argument('--maxK', type=int, required=True,
        help='Maximal number of factors.')
    
    consensus.add_argument('-nm', '--nmodels', type=int, required = True,
            help='Number of models to run per K.') 
    
    consensus.add_argument('-sc', '--supercons', default=False,
        help='Logical; false if running consensus for a single sample, true if across multiple samples. [Default False]')
    
    consensus.add_argument('--sim', default=False,
        help='Logical; false if clustering on A (path length = 1); true if clustering on A^2 (path length = 2). [Default False]')
        
    consensus.add_argument('--n_top_genes', default=1000,
            help='Top n genes with highest coefficient of variation. [Default 1000]')
    consensus.add_argument('-wt', '--weighting_type', default='jaccard',
            help='Walktrap weights, default uses jaccard similarity between adjacent nodes neighbors.')
    consensus.add_argument('-ct', '--cluster_type', default='walktrapP2',
            help='Algo of choise for community detection')
    consensus.add_argument('--steps', type=int, default=4,
            help='Walktrap step size. [Default 4]')
    consensus.add_argument('-dt',  '--density_threshold', type=float, default=2.0,
            help='cutoff to get rid of outlier factors (convergence criteria). [Default 2.0 not thresholding at all].')
    consensus.add_argument('-mcs', '--min_community_size', type=int, default=2,
            help='Number of clusters per minimal walktrap community '
            '[Default 2].')
    consensus.add_argument('--max_refit', type=int, default=150,
        help='Max refit trials. [Default 150]')

    ### Score command
    score = subparsers.add_parser('score',
            help='Create useful files such as gene scores, cell scores, and'
            ' ranked gene lists in txt format.')
    score.add_argument('-m', '--model', required=True,
            help='Saved scHPF model from train command. Should have extension' 
            '`.joblib`')
    score.add_argument('-o', '--outdir', default=None,
            help='Output directory for score files. If not given, a new'
            ' subdirectory of the dir containing the model will be made with'
            ' the same name as the model file (without extension)')
    score.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')
    score.add_argument('-g', '--genefile', default=None,
            help='Create an additional file with gene names ranked by score '
            'for each factor. Expects the gene.txt file output by the scHPF '
            'prep command or a similarly formatted tab-delimited file without '
            'headers. Uses the zero-indexed ``--name_col``\'th column as gene '
            'names. Optional.')
    score.add_argument('--name-col', type=int, default=1,
            help='The zero-indexed column of `genefile` to use as a gene name '
            'when (optionally) ranking genes. If ``--name_col`` is greater'
            ' than the index of ``--genefile``\'s last column, it is '
            ' automatically reset to the last column\'s index. [Default 1]'
        )



    # ###### Project command
    proj = subparsers.add_parser('project',
            help='Project new data onto a trained model.')
    # data and saving
    proj.add_argument('-m', '--model', required=True,
            help='The model to project onto.')
    proj.add_argument('-i', '--input', required=True,
            help='Data to project onto model. Expects either the mtx file'
            ' output by the prep or prep-like commands or a tab-delimitted'
            ' tsv file formated like: `CELL_ID\tGENE_ID\tUMI_COUNT`. In the'
            ' later case, ids are assumed to be 0 indexed and we assume no'
            ' duplicates.')
    proj.add_argument('-o', '--outdir', 
            help='Output directory for projected scHPF model. Will be created'
            ' if does not exist.')
    proj.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # projection-specific args
    proj.add_argument('--recalc-bp', action='store_true',
            help='Recalculate hyperparameter bp for the new data')

    # Training parameters (same as train, different defaults, no short names)
    proj.add_argument('--max-iter', type=int, default=500,
            help='Maximum iterations. [Default 500].')
    proj.add_argument('--min-iter', type=int, default=10,
            help='Minimum iterations. [Default 10]')
    proj.add_argument('--epsilon', type=float, default=0.001,
            help='Minimum percent decrease in loss between checks to continue '
            'inference (convergence criteria). [Default 0.001].')
    proj.add_argument('--check-freq', type=int, default=10,
            help='Number of iterations to run between convergence checks. '
            '[Default 10].')

    return parser


if __name__=='__main__':
    parser = _parser()
    args = parser.parse_args()

    # print help if no subparser given
    if len(sys.argv)==1:
        parser.print_help(sys.stderr)
        sys.exit(1)

    # setup paths and prefixes

    if args.outdir is None:
        if args.cmd in ['prep', 'prep-like', 'train', 'train-pool', 'multi-train', 'consensus']:
            args.outdir = args.input.rsplit('/', 1)[0]
        elif args.cmd=='project':
            args.outdir = args.model.rsplit('/',1)[0]
        elif args.cmd=='score':
            args.outdir = args.model.split('.joblib')[0]

    if args.outdir is not None and not os.path.exists(args.outdir):
        print("Creating output directory {} ".format(args.outdir))
        os.makedirs(args.outdir)
    prefix = args.prefix.rstrip('.') + '.' if len(args.prefix) > 0 else ''
    outprefix = args.outdir + '/' +  prefix

    if args.cmd == 'prep':
        filtered, genes = load_and_filter(args.input, 
                min_cells=args.min_cells, 
                whitelist=args.whitelist, 
                blacklist=args.blacklist, 
                filter_by_gene_name=args.filter_by_gene_name,
                no_split_on_dot=args.no_split_on_dot)

        print('Writing filtered data to file.....')
        mmwrite('{}filtered.mtx'.format(outprefix), filtered, field='integer')
        genes.to_csv('{}genes.txt'.format(outprefix), sep='\t', header=None,
                index=None)

        if args.n_validation_cells > 0:
            print('Selecting train/validation cells.....')
            Xtrn, Xvld, vld_ix = split_validation_cells( filtered,
                    args.n_validation_cells, args.validation_group_ids,
                    max_group_frac = args.validation_max_group_frac)
            trn_ix = np.setdiff1d(np.arange(filtered.shape[0]), vld_ix)

            print('Writing train/validation splits.....')
            mmwrite('{}train_cells.mtx'.format(outprefix), Xtrn, 
                    field='integer')
            np.savetxt('{}train_cell_ix.txt'.format(outprefix), trn_ix, 
                    fmt='%d')
            mmwrite('{}validation_cells.mtx'.format(outprefix), Xvld, 
                    field='integer')
            np.savetxt('{}validation_cell_ix.txt'.format(outprefix), vld_ix, 
                    fmt='%d')

        print('Writing commandline arguments to file.....')
        cmdfile = '{}prep_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)



    elif args.cmd == 'prep-like':
        print('Loading and reordering input like reference.... ')
        filtered, genes = load_like(args.input, reference=args.reference,
                by_gene_name=args.by_gene_name, 
                no_split_on_dot=args.no_split_on_dot)
        print('Writing prepared data to file.....')
        mmwrite('{}filtered.mtx'.format(outprefix), filtered, field='integer')
        genes.to_csv('{}genes.txt'.format(outprefix), sep='\t', header=None,
                index=None)
        print('Writing commandline arguments to file.....')
        cmdfile = '{}prep-like_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)


    elif args.cmd in ['train', 'multi-train', 'train-pool']:
        # load data
        print( 'Loading data.....' )
        load_fnc = mmread if args.input.endswith('.mtx') else load_coo
        train = load_fnc(args.input)

        ncells, ngenes = train.shape
        msg = '.....found {} cells and {} genes in {}'.format(
                ncells, ngenes, args.input)
        print(msg)

        if args.batchsize and ncells > args.batchsize and not args.reproject:
            msg = '\nWARNING: running with minibatches but without reproject.' \
                + ' We recommend adding the --reproject flag when running with'\
                + ' batches to synchronize cell variational distributions. \n'
            print(msg)

        if args.validation_cells is not None:
            vcells = load_fnc(args.validation_cells)
            msg = '.....found {} validation cells and {} genes in {}'.format(
                    vcells.shape[0], vcells.shape[1], args.validation_cells)
            print(msg)
            msg = 'WARNING: scHPF models with validation cells can be slow'
            msg += ' to converge.\n\tIf you observe this, try either (or both)'
            msg += ' increasing epsilon (-e, currently set to {})'.format(
                    args.epsilon)
            msg += ' or increasing the number of validation cells (using prep)'
            print(msg)
        else:
            vcells = None

        # create model
        print('Running trials.....' )
        dtype = np.float32 if args.float32 else np.float64
        model_kwargs = dict(a=args.a, c=args.c)

        if args.cmd in ['train','multi-train']:
            run_fnc = run_trials
        else:
            if args.njobs < 0:
                msg = 'njobs must be an int >= 0, received {}'
                raise ValueError(msg.format(args.njobs))
            run_fnc = partial(run_trials_pool, njobs=args.njobs)

        if args.cmd =='train':
            reject = None
            start_time = time.time()
            if args.save_all:
                model, reject = run_fnc(train, vcells=vcells, 
                            nfactors=args.nfactors, ntrials=args.ntrials,
                            min_iter=args.min_iter, max_iter=args.max_iter,
                            check_freq=args.check_freq, epsilon=args.epsilon,
                            better_than_n_ago=args.better_than_n_ago, dtype=dtype,
                            verbose=args.verbose, model_kwargs=model_kwargs,
                            return_all=True, reproject=args.reproject,
                            batchsize=args.batchsize,
                            beta_theta_simultaneous=args.beta_theta_simultaneous,
                            loss_smoothing=args.smooth_loss
                            )
            else:
                model = run_fnc(train, vcells=vcells, nfactors=args.nfactors, 
                            ntrials=args.ntrials, min_iter=args.min_iter,
                            max_iter=args.max_iter, check_freq=args.check_freq,
                            epsilon=args.epsilon,
                            better_than_n_ago=args.better_than_n_ago, dtype=dtype,
                            verbose=args.verbose, model_kwargs=model_kwargs,
                            return_all=False, reproject=args.reproject,
                            batchsize=args.batchsize,
                            beta_theta_simultaneous=args.beta_theta_simultaneous,
                            loss_smoothing=args.smooth_loss
                            )
            
            # save the model/models
            if isinstance(args.nfactors, int):
                klist = [args.nfactors]
                model = [model]
                if reject is not None:
                    reject = [reject]
            else:
                klist = args.nfactors
            for i, (K,m) in enumerate(zip(klist, model)):
                model_outprefix = '{}scHPF_K{}{}_{}trials'.format(
                        outprefix, K, 
                        f'_b{args.batchsize}' if ncells > args.batchsize else '', 
                        args.ntrials)
                if vcells is None:
                    print('Saving best model ({} factors).....'.format(K))
                    joblib.dump(m, model_outprefix + '.joblib')
                else:
                    print('Saving best model (training data, {} factors).....'\
                            .format(K))
                    joblib.dump(m, model_outprefix + '.train.joblib')

                    print('Computing final validation projection ({} factors)....'\
                            .format(K))
                    projection = m.project(vcells, replace=False)
                    print('Saving validation projection.....({} factors)'.format(K))
                    joblib.dump(projection, 
                            model_outprefix + '.validation_proj.joblib')
                if args.save_all:
                    for j,r in enumerate(reject[i]):
                        joblib.dump(r, model_outprefix + f'_reject{j+1}.joblib')
            duration = (time.time() - start_time) / 60
            print(f"Duration {duration} minutes. {model_outprefix}")

        elif args.cmd =='multi-train':
            if args.ntrials is not None and \
            args.minK is not None and \
            args.maxK is not None and \
            args.nmodels is not None:
                assert(args.njobs >= 0)
                run_fnc = partial(run_trials_pool, njobs=args.njobs)
                for k in range(args.minK,args.maxK+1):
                    start_time = time.time()
                    for m in range(args.nmodels):
                        sub='K'+str(k)+'_'+'n_'+str(m)+'/'
                        print(sub)
                        outprefix = args.outdir+sub
                        if not os.path.exists(outprefix):
                            os.mkdir(outprefix) 
                        model = run_fnc(train, vcells=vcells, nfactors=k, 
                                    ntrials=args.ntrials, min_iter=args.min_iter,
                                    max_iter=args.max_iter, check_freq=args.check_freq,
                                    epsilon=args.epsilon,
                                    better_than_n_ago=args.better_than_n_ago, dtype=dtype,
                                    verbose=args.verbose, model_kwargs=model_kwargs,
                                    return_all=False, reproject=args.reproject,
                                    batchsize=args.batchsize,
                                    beta_theta_simultaneous=args.beta_theta_simultaneous,
                                    loss_smoothing=args.smooth_loss
                                    )
                        # save the model/models
                        if isinstance(k, int):
                            klist = [k]
                            model = [model]

                        else:
                            klist = k
                        for i, (K,m) in enumerate(zip(klist, model)):
                            model_outprefix = '{}scHPF_K{}{}_{}trials'.format(
                                    outprefix, K, 
                                    f'_b{args.batchsize}' if ncells > args.batchsize else '', 
                                    args.ntrials)
                            if vcells is None:
                                print('Saving best model ({} factors).....'.format(K))
                                joblib.dump(m, model_outprefix + '.joblib')
                            else:
                                print('Saving best model (training data, {} factors).....'\
                                        .format(K))
                                joblib.dump(m, model_outprefix + '.train.joblib')

                                print('Computing final validation projection ({} factors)....'\
                                        .format(K))
                                projection = m.project(vcells, replace=False)
                                print('Saving validation projection.....({} factors)'.format(K))
                                joblib.dump(projection, 
                                        model_outprefix + '.validation_proj.joblib')
                            if args.save_all:
                                for j,r in enumerate(reject[i]):
                                    joblib.dump(r, model_outprefix + f'_reject{j+1}.joblib')
                        duration = (time.time() - start_time) / 60
                        print(f"Duration {duration} minutes. {model_outprefix}")


        print('Writing commandline arguments to file.....')
        cmdfile = '{}train_commandline_args.json'.format(args.outdir)
        print(cmdfile)
        if os.path.exists(cmdfile):
            cmdfile = '{}train_commandline_args.{}.json'.format(args.outdir, 
                    time.strftime("%Y%m%d-%H%M%S"))
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)

        print('\n')

    elif args.cmd == 'consensus':             
        matrixfile = args.input
        genes = args.genefile
        sample_folder_prefix = args.sample_folder_prefix
        supercons_outdir = args.outdir
        samples = ast.literal_eval(str(args.samples))
        print(type(samples))
        print('samples of interest: {}'.format(samples))
        sample_list_dir = args.sample_list_dir
        
        sim = args.sim
        supercons = args.supercons
        minK = args.minK
        maxK = args.maxK
        nmodels = args.nmodels
        n_per = args.nper
        n_top_genes = args.n_top_genes
        weighting_type = args.weighting_type
        cluster_type = args.cluster_type
        steps = args.steps
        density_threshold = args.density_threshold
        min_community_size = args.min_community_size
        max_refit = args.max_refit
        
        if sample_list_dir is not None:
            sample_list = np.loadtxt(sample_list_dir, delimiter='\t', dtype=object)

        sample_cons_outputs = list()
        for sample in samples:
            print('Running Consensus for sample: {} ...'.format(sample))
            sample_folder=sample_folder_prefix+sample
            my_models=model_selection(sample_folder=sample_folder,
                                      minK=minK,
                                      maxK=maxK,
                                      n_per=n_per,
                                      nmodels=nmodels,
                                      supercons = False)
            sample_cons_output = clustering(my_models=my_models,
                                            minK=minK,
                                            maxK=maxK,
                                            n_top_genes=n_top_genes,
                                            matrixfile=matrixfile,
                                            genes=genes,
                                            sample_folder=sample_folder,
                                            sim=sim, 
                                            weighting_type=weighting_type,
                                            cluster_type=cluster_type, 
                                            min_community_size=min_community_size,
                                            sample=sample,
                                            input_sample_list=sample_list,
                                            supercons = False)
            sample_cons_outputs.append(sample_cons_output)
            
            print('Writing commandline arguments to file.....')
            cmdfile = '{}consensus_commandline_args.json'.format(sample_folder+'/consensus/')
            with open(cmdfile, 'w') as f:
                json.dump(args.__dict__, f, indent=2)
            
        if supercons:
            print('Running Super consensus across samples ...')
            my_models,minK,maxK = model_selection(model_files=sample_cons_outputs,
                                                  minK=None,
                                                  maxK=None,
                                                  n_per=n_per,
                                                  nmodels=nmodels,
                                                  supercons=True)

            sample_folder = supercons_outdir
            sample_cons = clustering(my_models=my_models,
                                     minK=minK,
                                     maxK=maxK,
                                     n_top_genes=n_top_genes,
                                     matrixfile=matrixfile,
                                     genes=genes,
                                     sample_folder=sample_folder, 
                                     sim=sim,
                                     weighting_type=weighting_type,
                                     cluster_type=cluster_type, 
                                     min_community_size=min_community_size,
                                     sample=None,
                                     input_sample_list=None,
                                     supercons=True)

            print('Writing commandline arguments to file.....')
            cmdfile = '{}consensus_commandline_args.json'.format(sample_folder+'/consensus/')
            with open(cmdfile, 'w') as f:
                json.dump(args.__dict__, f, indent=2)



    
    elif args.cmd == 'score':
        print('Loading model.....')
        model = joblib.load(args.model)

        print('Calculating scores.....')
        cell_score = model.cell_score()
        gene_score = model.gene_score()

        print('Saving scores.....')
        np.savetxt(outprefix + 'cell_score.txt', cell_score, delimiter='\t')
        np.savetxt(outprefix + 'gene_score.txt', gene_score, delimiter='\t')

        print('Calculating mean cellscore fractions.....')
        frac_list = mean_cellscore_fraction_list(cell_score)
        with open(outprefix + 'mean_cellscore_fraction.txt', 'w') as h:
            h.write('nfactors\tmean_cellscore_fraction\n')
            for i,csf in enumerate(frac_list):
                h.write('{}\t{}\n'.format(i+1,csf))

        print('Calculating maximum pairwise overlaps.....')
        table = max_pairwise_table(gene_score, 
                ntop_list=[50,100,150,200,250,300,350,400,450,500])
        table.to_csv(outprefix + 'maximum_overlaps.txt', sep='\t', index=False)

        if args.genefile is not None:
            print('Ranking genes.....')
            # load and format gene file
            genes = np.loadtxt(args.genefile, delimiter='\t', dtype=str)
            if len(genes.shape) == 1:
                genes = genes[:,None]
            # get column to use for gene names
            last_col = genes.shape[1] - 1
            name_col = last_col if args.name_col > last_col else args.name_col
            print('.....using {}\'th column of genefile as gene label'.format(
                name_col))

            # rank the genes by gene_score
            ranks = np.argsort(gene_score, axis=0)[::-1]
            ranked_genes = []
            for i in range(gene_score.shape[1]):
                ranked_genes.append(genes[ranks[:,i], name_col])
            ranked_genes = np.stack(ranked_genes).T
            print('Saving ranked genes.....')
            np.savetxt(outprefix + 'ranked_genes.txt', ranked_genes, 
                    fmt="%s", delimiter='\t')

        print('Writing commandline arguments to file.....')
        cmdfile = '{}score_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2) 

    elif args.cmd == 'project':
        print('Loading reference model.....')
        model = joblib.load(args.model)
        print('Loading data.....')
        load_fnc = mmread if args.input.endswith('.mtx') else load_coo
        proj_data = load_fnc(args.input)
        print('Projecting data.....')
        projection = model.project(proj_data, replace=False, verbose=True,
                                   recalc_bp=args.recalc_bp,
                                   min_iter=args.min_iter, 
                                   max_iter=args.max_iter, 
                                   check_freq=args.check_freq, 
                                   epsilon=args.epsilon, )
        print('Saving projection.....')
        if args.recalc_bp:
            outprefix += '{}.'.format('recalc_bp')
        proj_out = '{}{}.proj.joblib'.format(outprefix, 
                args.model.rsplit('.',1)[0].split('/')[-1])
        joblib.dump(projection, proj_out)

        print('Writing commandline arguments to file.....')
        cmdfile = '{}project_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)
