#! /usr/bin/env python

import os
import sys
import argparse
import json
import time
from functools import partial
import yaml
from copy import deepcopy

import time

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")

import numpy as np
import pandas as pd
from scipy.io import mmread, mmwrite
from scipy.sparse import coo_matrix
from scipy.stats import sem
import joblib
import glob
from schpf import scHPF, run_trials, run_trials_pool
from schpf.util import max_pairwise_table, mean_cellscore_fraction_list, get_param_dfs, get_spectra, \
get_genescore_spectra, get_spectra_order, get_local_density, make_consensus_plot, \
refit_local_params, get_ranked_genes
from schpf.preprocessing import load_coo, load_and_filter, load_like
from schpf.preprocessing import split_validation_cells
from schpf.loss import projection_loss_function, mean_negative_pois_llh

import umap
import matplotlib as mpl
from matplotlib import pyplot as plt
plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] =42
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import gridspec

import plotly.express as px
import plotly.io as pio

import igraph as ig

import scipy.sparse as sp
from scipy.spatial.distance import squareform
#from sklearn.decomposition.nmf import non_negative_factorization
#from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn.metrics import silhouette_score
from sklearn.utils import sparsefuncs
import sklearn

def _parser():
    # usage = """scHPF <command> [<args>]

# The most commonly used scHPF commands are:
    # prep    Prepare data
    # train   Train a model from data
    # score   Get cell-scores, gene-scores, and other data

# Some advanced scHPF commands are:
    # prep-like    Prepare data with the same genes & order as other data
    # project      Project data onto a pre-trained model
    # train-pool   Train a model, parallelized at the level of trials
    # """
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='cmd')

    ### Preprocess command
    prep = subparsers.add_parser('prep',
            help='Prepare data for training')
    # data
    prep.add_argument('-i', '--input', required=True,
            help='Input data. Currently accepts either: (1) a whitespace-'
            'delimited gene by cell UMI count matrix with 2 leading columns '
            'of gene attributes (ENSEMBL_ID and GENE_NAME respectively), or '
            '(2) a loom file with at least one of the row attributes '
            '`Accession` or `Gene`, where `Accession` is an ENSEMBL id and '
            '`Gene` is the name.'
            )
    prep.add_argument('-o', '--outdir', 
            help='Output directory. Does not need to exist.')
    prep.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # gene filtering criteria
    prep.add_argument('-m', '--min-cells', type=float, default=0.01, 
            help='Minimum number of cells in which we must observe at '
            'least one transcript of a gene for the gene to pass '
            'filtering. If 0 <`min_cells`< 1, sets threshold to be '
            '`min_cells` * ncells, rounded to the nearest integer.'
            ' [Default 0.01]')
    prep.add_argument('-w', '--whitelist', default='',
            help='Tab-delimited file where first column contains ENSEMBL gene '
            'ids to accept, and second column contains corresponding gene '
            'names. If given, genes not on the whitelist are filtered from '
            'the input matrix. Superseded by blacklist. Optional.')
    prep.add_argument('-b', '--blacklist', default='',
            help='Tab-delimited file where first column contains ENSEMBL gene '
            'ids to exclude, and second column is the corresponding gene name. '
            'Only performed if file given. Genes on the blacklist are '
            'excluded even if they are also on the whitelist. Optional.')

    # optional selection of cells for validation set
    prep.add_argument('-nvc', '--n-validation-cells', type=int, default=0,
            help='Number of cells to randomly select for validation.')
    prep.add_argument('-vgid', '--validation-group-ids', default=None,
            help= 'Single column file of cell group ids readable with '
            ' np.readtxt. If `--n-validation-cells` is > 0, cells will be '
            ' randomly selected approximately evenly across the groups in '
            ' this file, under the constraint that at most'
            ' `--validation-min-group-frac` * (ncells in group) are selected'
            ' from every group.')
    prep.add_argument('--validation-max-group-frac', type=float, default=0.5,
            help='If `-nvc`>0 and `validation-group-ids` is a valid file, at'
            ' most `validation-min-group-frac`*(ncells in group) cells are'
            ' selected from each group.')

    # other options
    prep.add_argument('--filter-by-gene-name', default=False, 
            action='store_true', help='Use gene name rather than ENSEMBL'
            ' id to filter (with whitelist or blacklist).  Useful for'
            ' datasets where only gene symbols are given. Applies to both'
            ' whitelist and blacklist. Used by default when input is a loom'
            ' file (unless there is an Accession attribute in the loom).')
    prep.add_argument('--no-split-on-dot', default=False, action='store_true',
            help='Don\'t split gene symbol or name on period before '
            'filtering whitelist and blacklist. We do this by default for '
            'ENSEMBL ids.')


    #### Prepare like
    prep_like = subparsers.add_parser('prep-like', 
            help='Prepare a data set like another (ie with the same genes in'
            ' the same order)')
    # data
    prep_like.add_argument('-i', '--input', required=True,
            help='Input data to format. Currently accepts either: (1) a'
            ' whitespace-delimited gene by cell UMI count matrix with 2'
            ' leading columns of gene attributes (ENSEMBL_ID and GENE_NAME'
            ' respectively), or (2) a loom file with at least one of the row'
            ' attributes `Accession` or `Gene`, where `Accession` is an'
            ' ENSEMBL id and `Gene` is the name.')
    prep_like.add_argument('-r', '--reference', required=True,
            help='Two-column tab-delimited file of ENSEMBL ids and gene names'
            ' to select from `input` and order like. All genes in `reference`'
            ' must be present in `input`.')
    prep_like.add_argument('-o', '--outdir', required=True,
            help='Output directory. Does not need to exist.')
    prep_like.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')
    # other options
    prep_like.add_argument('--by-gene-name', default=False, 
            action='store_true', help='Use gene name rather than ENSEMBL'
            ' id to when matching against reference.  Useful for datasets'
            ' where only gene symbols are given. Used by default when input'
            ' is a loom file (unless there is an Accession attr in the loom).')
    prep_like.add_argument('--no-split-on-dot', default=False, action='store_true',
            help='Don\'t split gene symbol or name on period before'
            ' when matching to reference. We do this by default for ENSEMBL'
            ' ids.')


    ###### Train command
    train = subparsers.add_parser('train',
            help='Train a model with automatic parallelization across'
            ' computations with numba')
    # data and saving
    train.add_argument('-i', '--input', required=True,
            help="Training data. Expects either the mtx file output by the "
            "prep command or a tab-separated tsv file formatted like:" 
            "`CELL_ID\tGENE_ID\tUMI_COUNT`. In the later case, ids are "
            "assumed to be 0 indexed and we assume no duplicates."
            )
    train.add_argument('-o', '--outdir', 
            help='Output directory for scHPF model. Will be created if does '
            'not exist.')
    train.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # Required model hyperparameter
    train.add_argument('-k', '--nfactors', type=int, required=True,
            help='Number of factors.')

    # training parameters
    train.add_argument('-t', '--ntrials',  type=int, default=1,
            help='Number of times to run scHPF, selecting the trial with '
            'best loss (on training data unless validation is given).'
            ' [Default 1]')
    train.add_argument('-v', '--validation-cells', default=None,
            help='Cells to use to assess convergence and choose a model.'
            ' Expects same format as ``-i/--input``. Training data used by'
            ' default.'
            )
    train.add_argument('-M', '--max-iter', type=int, default=1000,
            help='Maximum iterations. [Default 1000].')
    train.add_argument('-m', '--min-iter', type=int, default=30,
            help='Minimum iterations. [Default 30]')
    train.add_argument('-e',  '--epsilon', type=float, default=0.001,
            help='Minimum percent decrease in loss between checks to continue '
            'inference (convergence criteria). [Default 0.001].')
    train.add_argument('-f', '--check-freq', type=int, default=10,
            help='Number of iterations to run between convergence checks. '
            '[Default 10].')
    train.add_argument('--better-than-n-ago', default=5, type=int,
            help= 'Stop condition if loss is getting worse.  Stops training '
            'if loss is worse than `better_than_n_ago`*`check-freq` training '
            'steps ago and getting worse. Normally not necessary to change.')
    train.add_argument('-a', type=float, default=0.3,
            help='Value for hyperparameter a. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    train.add_argument('-c', type=float, default=0.3,
            help='Value for hyperparameter c. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    train.add_argument('--float32', action='store_true',
            help="Use 32-bit floats instead of default 64-bit floats in"
            " variational distrubtions")
    train.add_argument('-bs', '--batchsize', default=0, type=int,
            help="Number of cells to use per training round. All cells used if"
            " 0. Note that using batches changes the order of updates during"
            " inference.")
    train.add_argument('-sl', '--smooth-loss', default=1, type=int,
            help="Average loss over the last `--smooth-loss` interations."
            " Intended for when using minibatches, where int(ncells/batchsize)"
            " is a reasonable value"
            )
    train.add_argument('-bts', '--beta-theta-simultaneous', action='store_true',
            help="If False (default), compute beta update, then compute theta"
            " based on the updated beta. Note that if batching is used, this"
            " order is reverse. If True, update both beta and theta based on"
            " values from the last training round. The later slows the rate of"
            " convergence and sometimes results in better log-likelihoods, but"
            " may increase convergence time, especially for large numbers of"
            " cells."
            )
    train.add_argument('-sa', '--save-all', action='store_true',
            help="Save all trials")
    train.add_argument('-rp', '--reproject', action='store_true',
            help="Reproject data onto fixed global (gene) parameters after"
            " convergence, but before model selection. Recommended with"
            " batching")
    train.add_argument('--quiet', dest='verbose', action='store_false', 
            default=True, help="Don't print intermediate llh.")

    ###### train with trials in threadpool
    train_pool = subparsers.add_parser('train-pool', parents=[train], 
            add_help=False, conflict_handler='resolve')
    train_pool.add_argument('--njobs', type=int, default=0,
            help='Max number of processes to spawn. 0 will use the minimum of'
            ' all available cores and ntrials.')
    # Required model hyperparameter
    train_pool.add_argument('-k', '--nfactors', nargs='+', type=int, 
        required=True, help='Number of factors.')

    ###### Multiple train command
    multi_train = subparsers.add_parser('multi-train',
            help='[MC] automated training process through a series of K values')
    # data and saving
    multi_train.add_argument('-i', '--input', required=True,
            help="Training data. Expects either the mtx file output by the "
            "prep command or a tab-separated tsv file formatted like:" 
            "`CELL_ID\tGENE_ID\tUMI_COUNT`. In the later case, ids are "
            "assumed to be 0 indexed and we assume no duplicates."
            )
    multi_train.add_argument('-o', '--outdir', 
            help='Output directory for scHPF model. Will be created if does '
            'not exist.')
    multi_train.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # Required model hyperparameter
    multi_train.add_argument('--minK', type=int, required=True,
            help='Minimal number of factors.')
    
    multi_train.add_argument('--maxK', type=int, required=True,
        help='Maximal number of factors.')
    
    multi_train.add_argument('-nm', '--nmodels', type=int, required = True,
            help='Number of Models to run per K.')    
    multi_train.add_argument('--njobs', type=int, default=0,
            help='Max number of processes to spawn. 0 will use the minimum of'
            ' all available cores and ntrials.')
        
    # training parameters
    multi_train.add_argument('-t', '--ntrials',  type=int, default=1,
            help='Number of times to run scHPF, selecting the trial with '
            'best loss (on training data unless validation is given).'
            ' [Default 1]')
    multi_train.add_argument('-v', '--validation-cells', default=None,
            help='Cells to use to assess convergence and choose a model.'
            ' Expects same format as ``-i/--input``. Training data used by'
            ' default.'
            )
    multi_train.add_argument('-M', '--max-iter', type=int, default=1000,
            help='Maximum iterations. [Default 1000].')
    multi_train.add_argument('-m', '--min-iter', type=int, default=30,
            help='Minimum iterations. [Default 30]')
    multi_train.add_argument('-e',  '--epsilon', type=float, default=0.001,
            help='Minimum percent decrease in loss between checks to continue '
            'inference (convergence criteria). [Default 0.001].')
    multi_train.add_argument('-f', '--check-freq', type=int, default=10,
            help='Number of iterations to run between convergence checks. '
            '[Default 10].')
    multi_train.add_argument('--better-than-n-ago', default=5, type=int,
            help= 'Stop condition if loss is getting worse.  Stops training '
            'if loss is worse than `better_than_n_ago`*`check-freq` training '
            'steps ago and getting worse. Normally not necessary to change.')
    multi_train.add_argument('-a', type=float, default=0.3,
            help='Value for hyperparameter a. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    multi_train.add_argument('-c', type=float, default=0.3,
            help='Value for hyperparameter c. Setting to -2 will auto-set to'
            ' 1/sqrt(nfactors)[Default 0.3]')
    multi_train.add_argument('--float32', action='store_true',
            help="Use 32-bit floats instead of default 64-bit floats in"
            " variational distrubtions")
    multi_train.add_argument('-bs', '--batchsize', default=0, type=int,
            help="Number of cells to use per training round. All cells used if"
            " 0. Note that using batches changes the order of updates during"
            " inference.")
    multi_train.add_argument('-sl', '--smooth-loss', default=1, type=int,
            help="Average loss over the last `--smooth-loss` interations."
            " Intended for when using minibatches, where int(ncells/batchsize)"
            " is a reasonable value"
            )
    multi_train.add_argument('-bts', '--beta-theta-simultaneous', action='store_true',
            help="If False (default), compute beta update, then compute theta"
            " based on the updated beta. Note that if batching is used, this"
            " order is reverse. If True, update both beta and theta based on"
            " values from the last training round. The later slows the rate of"
            " convergence and sometimes results in better log-likelihoods, but"
            " may increase convergence time, especially for large numbers of"
            " cells."
            )
    multi_train.add_argument('-sa', '--save-all', action='store_true',
            help="Save all trials")
    multi_train.add_argument('-rp', '--reproject', action='store_true',
            help="Reproject data onto fixed global (gene) parameters after"
            " convergence, but before model selection. Recommended with"
            " batching")
    multi_train.add_argument('--quiet', dest='verbose', action='store_false', 
            default=True, help="Don't print intermediate llh.")

    ###### train with trials in threadpool
    multi_train_pool = subparsers.add_parser('multi-train-pool', parents=[multi_train], 
            add_help=False, conflict_handler='resolve')
    multi_train_pool.add_argument('--njobs', type=int, default=0,
            help='Max number of processes to spawn. 0 will use the minimum of'
            ' all available cores and ntrials.')

    ###### Super consensus command
    consensus = subparsers.add_parser('consensus',
            help='[MC] super consensus')
    # data and saving
    consensus.add_argument('-i', '--input', required=True,
        help="Training data. Expects either the mtx file output by the "
        "prep command or a tab-separated tsv file formatted like:" 
        "`CELL_ID\tGENE_ID\tUMI_COUNT`. In the later case, ids are "
        "assumed to be 0 indexed and we assume no duplicates."
        )
    consensus.add_argument('-o', '--outdir', 
            help='Output directory for scHPF model(s). Will be created if does '
            'not exist.')
    consensus.add_argument('--nper', type=int, default=3,
            help='Top n models with minimal loss. [Default 3]')

    # Required model hyperparameter
    consensus.add_argument('--minK', type=int, required=True,
            help='Minimal number of factors.')
    
    consensus.add_argument('--maxK', type=int, required=True,
        help='Maximal number of factors.')
    
    consensus.add_argument('-nm', '--nmodels', type=int, required = True,
            help='Number of Models to run per K.') 
    consensus.add_argument('-p', '--prefix', default='',
        help='Prefix for output files. Optional.')

        
    # sub parameters
    consensus.add_argument('--ngenes', default=1000,
            help='Top n genes with highest coefficient of variation. [Default 1000]')
    consensus.add_argument('-wt', '--weighting_type', default='jaccard',
            help='Walktrap weights, default uses jaccard similarity between adjacent nodes neighbors.')
    consensus.add_argument('-ct', '--cluster_type', default='walktrapP2',
            help='Algo of choise for community detection')
    consensus.add_argument('--steps', type=int, default=4,
            help='Walktrap step size. [Default 4]')
    consensus.add_argument('-dt',  '--density_threshold', type=float, default=2.0,
            help='cutoff to get rid of outlier factors (convergence criteria). [Default 2.0 not thresholding at all].')
    consensus.add_argument('-mcs', '--min_community_size', type=int, default=2,
            help='Number of clusters per minimal walktrap community '
            '[Default 2].')
 
    ### Score command
    score = subparsers.add_parser('score',
            help='Create useful files such as gene scores, cell scores, and'
            ' ranked gene lists in txt format.')
    score.add_argument('-m', '--model', required=True,
            help='Saved scHPF model from train command. Should have extension' 
            '`.joblib`')
    score.add_argument('-o', '--outdir', default=None,
            help='Output directory for score files. If not given, a new'
            ' subdirectory of the dir containing the model will be made with'
            ' the same name as the model file (without extension)')
    score.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')
    score.add_argument('-g', '--genefile', default=None,
            help='Create an additional file with gene names ranked by score '
            'for each factor. Expects the gene.txt file output by the scHPF '
            'prep command or a similarly formatted tab-delimited file without '
            'headers. Uses the zero-indexed ``--name_col``\'th column as gene '
            'names. Optional.')
    score.add_argument('--name-col', type=int, default=1,
            help='The zero-indexed column of `genefile` to use as a gene name '
            'when (optionally) ranking genes. If ``--name_col`` is greater'
            ' than the index of ``--genefile``\'s last column, it is '
            ' automatically reset to the last column\'s index. [Default 1]'
        )



    # ###### Project command
    proj = subparsers.add_parser('project',
            help='Project new data onto a trained model.')
    # data and saving
    proj.add_argument('-m', '--model', required=True,
            help='The model to project onto.')
    proj.add_argument('-i', '--input', required=True,
            help='Data to project onto model. Expects either the mtx file'
            ' output by the prep or prep-like commands or a tab-delimitted'
            ' tsv file formated like: `CELL_ID\tGENE_ID\tUMI_COUNT`. In the'
            ' later case, ids are assumed to be 0 indexed and we assume no'
            ' duplicates.')
    proj.add_argument('-o', '--outdir', 
            help='Output directory for projected scHPF model. Will be created'
            ' if does not exist.')
    proj.add_argument('-p', '--prefix', default='',
            help='Prefix for output files. Optional.')

    # projection-specific args
    proj.add_argument('--recalc-bp', action='store_true',
            help='Recalculate hyperparameter bp for the new data')

    # Training parameters (same as train, different defaults, no short names)
    proj.add_argument('--max-iter', type=int, default=500,
            help='Maximum iterations. [Default 500].')
    proj.add_argument('--min-iter', type=int, default=10,
            help='Minimum iterations. [Default 10]')
    proj.add_argument('--epsilon', type=float, default=0.001,
            help='Minimum percent decrease in loss between checks to continue '
            'inference (convergence criteria). [Default 0.001].')
    proj.add_argument('--check-freq', type=int, default=10,
            help='Number of iterations to run between convergence checks. '
            '[Default 10].')

    return parser


if __name__=='__main__':
    parser = _parser()
    args = parser.parse_args()

    # print help if no subparser given
    if len(sys.argv)==1:
        parser.print_help(sys.stderr)
        sys.exit(1)

    # setup paths and prefixes

    if args.outdir is None:
        if args.cmd in ['prep', 'prep-like', 'train', 'train-pool', 'multi-train', 'consensus']:
            args.outdir = args.input.rsplit('/', 1)[0]
        elif args.cmd=='project':
            args.outdir = args.model.rsplit('/',1)[0]
        elif args.cmd=='score':
            args.outdir = args.model.split('.joblib')[0]

    if args.outdir is not None and not os.path.exists(args.outdir):
        print("Creating output directory {} ".format(args.outdir))
        os.makedirs(args.outdir)
    prefix = args.prefix.rstrip('.') + '.' if len(args.prefix) > 0 else ''
    outprefix = args.outdir + '/' +  prefix

    if args.cmd == 'prep':
        filtered, genes = load_and_filter(args.input, 
                min_cells=args.min_cells, 
                whitelist=args.whitelist, 
                blacklist=args.blacklist, 
                filter_by_gene_name=args.filter_by_gene_name,
                no_split_on_dot=args.no_split_on_dot)

        print('Writing filtered data to file.....')
        mmwrite('{}filtered.mtx'.format(outprefix), filtered, field='integer')
        genes.to_csv('{}genes.txt'.format(outprefix), sep='\t', header=None,
                index=None)

        if args.n_validation_cells > 0:
            print('Selecting train/validation cells.....')
            Xtrn, Xvld, vld_ix = split_validation_cells( filtered,
                    args.n_validation_cells, args.validation_group_ids,
                    max_group_frac = args.validation_max_group_frac)
            trn_ix = np.setdiff1d(np.arange(filtered.shape[0]), vld_ix)

            print('Writing train/validation splits.....')
            mmwrite('{}train_cells.mtx'.format(outprefix), Xtrn, 
                    field='integer')
            np.savetxt('{}train_cell_ix.txt'.format(outprefix), trn_ix, 
                    fmt='%d')
            mmwrite('{}validation_cells.mtx'.format(outprefix), Xvld, 
                    field='integer')
            np.savetxt('{}validation_cell_ix.txt'.format(outprefix), vld_ix, 
                    fmt='%d')

        print('Writing commandline arguments to file.....')
        cmdfile = '{}prep_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)



    elif args.cmd == 'prep-like':
        print('Loading and reordering input like reference.... ')
        filtered, genes = load_like(args.input, reference=args.reference,
                by_gene_name=args.by_gene_name, 
                no_split_on_dot=args.no_split_on_dot)
        print('Writing prepared data to file.....')
        mmwrite('{}filtered.mtx'.format(outprefix), filtered, field='integer')
        genes.to_csv('{}genes.txt'.format(outprefix), sep='\t', header=None,
                index=None)
        print('Writing commandline arguments to file.....')
        cmdfile = '{}prep-like_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)


    elif args.cmd in ['train', 'multi-train', 'train-pool']:
        # load data
        print( 'Loading data.....' )
        load_fnc = mmread if args.input.endswith('.mtx') else load_coo
        train = load_fnc(args.input)

        ncells, ngenes = train.shape
        msg = '.....found {} cells and {} genes in {}'.format(
                ncells, ngenes, args.input)
        print(msg)

        if args.batchsize and ncells > args.batchsize and not args.reproject:
            msg = '\nWARNING: running with minibatches but without reproject.' \
                + ' We recommend adding the --reproject flag when running with'\
                + ' batches to synchronize cell variational distributions. \n'
            print(msg)

        if args.validation_cells is not None:
            vcells = load_fnc(args.validation_cells)
            msg = '.....found {} validation cells and {} genes in {}'.format(
                    vcells.shape[0], vcells.shape[1], args.validation_cells)
            print(msg)
            msg = 'WARNING: scHPF models with validation cells can be slow'
            msg += ' to converge.\n\tIf you observe this, try either (or both)'
            msg += ' increasing epsilon (-e, currently set to {})'.format(
                    args.epsilon)
            msg += ' or increasing the number of validation cells (using prep)'
            print(msg)
        else:
            vcells = None

        # create model
        print('Running trials.....' )
        dtype = np.float32 if args.float32 else np.float64
        model_kwargs = dict(a=args.a, c=args.c)

        if args.cmd in ['train','multi-train']:
            run_fnc = run_trials
        else:
            if args.njobs < 0:
                msg = 'njobs must be an int >= 0, received {}'
                raise ValueError(msg.format(args.njobs))
            run_fnc = partial(run_trials_pool, njobs=args.njobs)

        if args.cmd =='train':
            reject = None
            start_time = time.time()
            if args.save_all:
                model, reject = run_fnc(train, vcells=vcells, 
                            nfactors=args.nfactors, ntrials=args.ntrials,
                            min_iter=args.min_iter, max_iter=args.max_iter,
                            check_freq=args.check_freq, epsilon=args.epsilon,
                            better_than_n_ago=args.better_than_n_ago, dtype=dtype,
                            verbose=args.verbose, model_kwargs=model_kwargs,
                            return_all=True, reproject=args.reproject,
                            batchsize=args.batchsize,
                            beta_theta_simultaneous=args.beta_theta_simultaneous,
                            loss_smoothing=args.smooth_loss
                            )
            else:
                model = run_fnc(train, vcells=vcells, nfactors=args.nfactors, 
                            ntrials=args.ntrials, min_iter=args.min_iter,
                            max_iter=args.max_iter, check_freq=args.check_freq,
                            epsilon=args.epsilon,
                            better_than_n_ago=args.better_than_n_ago, dtype=dtype,
                            verbose=args.verbose, model_kwargs=model_kwargs,
                            return_all=False, reproject=args.reproject,
                            batchsize=args.batchsize,
                            beta_theta_simultaneous=args.beta_theta_simultaneous,
                            loss_smoothing=args.smooth_loss
                            )
            
            # save the model/models
            if isinstance(args.nfactors, int):
                klist = [args.nfactors]
                model = [model]
                if reject is not None:
                    reject = [reject]
            else:
                klist = args.nfactors
            for i, (K,m) in enumerate(zip(klist, model)):
                model_outprefix = '{}scHPF_K{}{}_{}trials'.format(
                        outprefix, K, 
                        f'_b{args.batchsize}' if ncells > args.batchsize else '', 
                        args.ntrials)
                if vcells is None:
                    print('Saving best model ({} factors).....'.format(K))
                    joblib.dump(m, model_outprefix + '.joblib')
                else:
                    print('Saving best model (training data, {} factors).....'\
                            .format(K))
                    joblib.dump(m, model_outprefix + '.train.joblib')

                    print('Computing final validation projection ({} factors)....'\
                            .format(K))
                    projection = m.project(vcells, replace=False)
                    print('Saving validation projection.....({} factors)'.format(K))
                    joblib.dump(projection, 
                            model_outprefix + '.validation_proj.joblib')
                if args.save_all:
                    for j,r in enumerate(reject[i]):
                        joblib.dump(r, model_outprefix + f'_reject{j+1}.joblib')
            duration = (time.time() - start_time) / 60
            print(f"Duration {duration} minutes. {model_outprefix}")

        elif args.cmd =='multi-train':
            if args.ntrials is not None and \
            args.minK is not None and \
            args.maxK is not None and \
            args.nmodels is not None:
                assert(args.njobs >= 0)
                run_fnc = partial(run_trials_pool, njobs=args.njobs)
                for k in range(args.minK,args.maxK+1):
                    start_time = time.time()
                    for m in range(args.nmodels):
                        sub='K'+str(k)+'_'+'n_'+str(m)+'/'
                        print(sub)
                        outprefix = args.outdir+sub
                        if not os.path.exists(outprefix):
                            os.mkdir(outprefix) 
                        model = run_fnc(train, vcells=vcells, nfactors=k, 
                                    ntrials=args.ntrials, min_iter=args.min_iter,
                                    max_iter=args.max_iter, check_freq=args.check_freq,
                                    epsilon=args.epsilon,
                                    better_than_n_ago=args.better_than_n_ago, dtype=dtype,
                                    verbose=args.verbose, model_kwargs=model_kwargs,
                                    return_all=False, reproject=args.reproject,
                                    batchsize=args.batchsize,
                                    beta_theta_simultaneous=args.beta_theta_simultaneous,
                                    loss_smoothing=args.smooth_loss
                                    )
                        # save the model/models
                        if isinstance(k, int):
                            klist = [k]
                            model = [model]

                        else:
                            klist = k
                        for i, (K,m) in enumerate(zip(klist, model)):
                            model_outprefix = '{}scHPF_K{}{}_{}trials'.format(
                                    outprefix, K, 
                                    f'_b{args.batchsize}' if ncells > args.batchsize else '', 
                                    args.ntrials)
                            if vcells is None:
                                print('Saving best model ({} factors).....'.format(K))
                                joblib.dump(m, model_outprefix + '.joblib')
                            else:
                                print('Saving best model (training data, {} factors).....'\
                                        .format(K))
                                joblib.dump(m, model_outprefix + '.train.joblib')

                                print('Computing final validation projection ({} factors)....'\
                                        .format(K))
                                projection = m.project(vcells, replace=False)
                                print('Saving validation projection.....({} factors)'.format(K))
                                joblib.dump(projection, 
                                        model_outprefix + '.validation_proj.joblib')
                            if args.save_all:
                                for j,r in enumerate(reject[i]):
                                    joblib.dump(r, model_outprefix + f'_reject{j+1}.joblib')
                        duration = (time.time() - start_time) / 60
                        print(f"Duration {duration} minutes. {model_outprefix}")


        print('Writing commandline arguments to file.....')
        cmdfile = '{}train_commandline_args.json'.format(args.outdir)
        print(cmdfile)
        if os.path.exists(cmdfile):
            cmdfile = '{}train_commandline_args.{}.json'.format(args.outdir, 
                    time.strftime("%Y%m%d-%H%M%S"))
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)

        print('\n')

    elif args.cmd == 'consensus':
        MODEL_DIR = args.outdir
        N_PER = args.nper
        NMODELS=args.nmodels
        OUTDIR = args.outdir+'super_consensus/'
        GENES = args.outdir+'genes.txt'
        minK = args.minK
        maxK = args.maxK
        N_TOP_GENES = args.ngenes
        WEIGHTING_TYPE = args.weighting_type
        CLUSTER_TYPE = args.cluster_type
        STEPS = args.steps
        DENSITY_THRESHOLD = args.density_threshold
        MIN_COMMUNITY_SIZE = args.min_community_size

        # load data
        print( 'Loading data.....' )
        load_fnc = mmread if args.input.endswith('.mtx') else load_coo
        X = load_fnc(args.input)
        
        if not os.path.exists(OUTDIR):
            os.mkdir(OUTDIR)
        genes = np.loadtxt(GENES, delimiter='\t', dtype=str)

        if NMODELS <= 10:
            MODEL_TEMPLATE = MODEL_DIR+'K[1-9][0-9]_n_[0-9]/scHPF*.joblib'
        elif NMODELS > 10 and NMODELS <=100:
            MODEL_TEMPLATE = MODEL_DIR+'K[1-9][0-9]_n_[0-9][0-9]/scHPF*.joblib'

        model_files = sorted(glob.glob(MODEL_TEMPLATE))
        
        Ks = [int(m.split('scHPF_K')[1].split('_')[0]) for m in model_files]
        models = [joblib.load(m) for m in model_files]
        losses = np.array([m.loss[-1] for m in models]).reshape(len(range(minK,maxK+1)),NMODELS)
        
        model_files_mtx = np.array([n for n in model_files]).reshape(len(range(minK,maxK+1)),NMODELS)
        n_reject = NMODELS - N_PER
        rejected_models = []
        for i in range(model_files_mtx.shape[0]):
            rejected = sorted(zip(losses[i,:], model_files_mtx[i,:]), reverse=False)[:n_reject]
            for m in rejected:
                rejected_models.append(m[1]) 
        rejected_models = sorted(rejected_models)
        names = [m.split('/')[-1].rsplit('.',-1)[0] if m not in rejected_models 
                 else m.split('/')[-1].rsplit('.',-1)[0]+'reject'
                 for m in model_files]

        for m,n in zip(models, names):
            m.name = n

       # get top N_PER models for each K
        my_models = [m for m in models 
                     if ('reject' not in m.name)# or int(m.name.split('reject')[1][0])<N_PER)
                        and m.nfactors >= minK
                        and m.nfactors <= maxK 
                    ]

        minK = np.min([m.nfactors for m in my_models])
        maxK = np.max([m.nfactors for m in my_models])
        print('n_models:', len(my_models), 'minK:', minK, 'maxK:', maxK, 'n_per', N_PER)
        assert(len(my_models) == (maxK - minK + 1)*N_PER)
        

        eta_shp, eta_rte, beta_shp, beta_rte = get_spectra(my_models)
        eta_e_x = eta_shp/eta_rte
        beta_e_x = beta_shp/beta_rte

        #spectra = beta_e_x
        spectra = get_genescore_spectra(my_models)
        gene_ixs = (spectra.std()/spectra.mean()).astype(float).nlargest(int(N_TOP_GENES)).index.values # ordered genes by decreasingcoefficients of variation
        #print(len(gene_ixs))
        transformed_spectra = spectra[gene_ixs]
        #transformed_spectra = l2_norm(spectra[gene_ixs])
        #transformed_spectra = pd.DataFrame(StandardScaler().fit_transform(spectra[gene_ixs]), index=spectra.index)


        factor_dists = 1 - pd.DataFrame(transformed_spectra, index=transformed_spectra.index).T.corr().values
    #     type(factor_dists) # a numpy ndarray of total n of factors by total n of factors 

        reducer = umap.UMAP(metric='precomputed', random_state=0) #, n_neighbors=n_neighbors)
        embedding = reducer.fit_transform(factor_dists)

        local_neighborhood_size = 0.25  # just a heuristic for selecting k
        n_neighbors = max(5, int(local_neighborhood_size * len(my_models)))
        print(f'{n_neighbors}/{transformed_spectra.shape[0]}')

        local_density = get_local_density(factor_dists, n_neighbors, transformed_spectra.index)
        print(min(local_density['local_density']))



        # get neighbors
        sim = False
        adj_binary = sklearn.neighbors.kneighbors_graph(factor_dists, n_neighbors, 
                                                        metric='precomputed',
                                                        )
        if sim:
            adj_binary = adj_binary.multiply(adj_binary.T)

        if WEIGHTING_TYPE in ['jaccard','jaccard2']:
            adj = np.zeros(adj_binary.shape)
            for i,j in np.stack(adj_binary.nonzero()).T:
                adj[i,j] = metrics.jaccard_score(adj_binary[i,:].A[0], adj_binary[j,:].A[0])
                if WEIGHTING_TYPE=='jaccard2':
                    adj[i,j] = max(adj[i,j], 1e-5)
            adj = sp.coo_matrix(adj)
        elif WEIGHTING_TYPE == 'clipCorr':
            # (must convert dists to non-neg connectivities)
            adj = sp.coo_matrix(adj_binary.A * np.clip(1-factor_dists,0,None)) #
        elif WEIGHTING_TYPE == 'shiftCorr':
            adj = sp.coo_matrix(adj_binary.A * (2-factor_dists))
        else:
            assert False



        np.random.seed(0)

        if STEPS is None: 
            STEPS = 4

        vcount = max(adj.shape)
        sources, targets = adj.nonzero()
        edgelist = list(zip(sources.tolist(), targets.tolist()))

        if CLUSTER_TYPE is None:
            CLUSTER_TYPE = 'walktrapP2'

        if CLUSTER_TYPE.startswith('walktrap'):
            knn = ig.Graph(edges=edgelist, directed=False)
            knn.vs['label'] = transformed_spectra.index
            knn.es['width'] = adj.data
            knn.es['weight'] = adj.data
            # note 4 is the default 
            cluster_result = knn.community_walktrap(weights=adj.data, steps=STEPS)
            if CLUSTER_TYPE == 'walktrapP1':
                nclusters = cluster_result.optimal_count + 1
            elif CLUSTER_TYPE == 'walktrapP2':
                nclusters = cluster_result.optimal_count + 2
            else:
                nclusters = cluster_result.optimal_count
            cluster_labels = pd.Series(cluster_result.as_clustering(nclusters).membership, index=transformed_spectra.index)
            print(f'Nclusters: {len(cluster_labels.unique())} ({cluster_result.optimal_count})')
        elif CLUSTER_TYPE == 'leidenA':
            import leidenalg
            knn = ig.Graph(edges=edgelist, directed=True)
            knn.vs['label'] = transformed_spectra.index
            #knn.es['width'] = adj.data
            knn.es['weight'] = adj.data
            cluster_result = leidenalg.find_partition(knn, #leidenalg.ModularityVertexPartition,
                                                      leidenalg.RBConfigurationVertexPartition,
                                                      #leidenalg.CPMVertexPartition,
                                                      resolution_parameter=1,
                                                      weights=adj.data, n_iterations=-1, seed=0,
                                                    )
            #cluster_result = knn.community_leiden(objective_function="modularity",
            #                                      #resolution_parameter=,
            #                                      weights=adj.data)
            cluster_labels = pd.Series(cluster_result.membership, index=transformed_spectra.index)
            #cluster_labels = pd.Series(cluster_result, index=transformed_spectra.index)
            print(len(np.unique(cluster_result.membership)))
        else:
            assert False



        optimal_count = cluster_result.optimal_count
        x = np.arange(-5,10)
        modularity = [cluster_result.as_clustering(optimal_count + i).modularity for i in x]

        plt.plot(x + optimal_count, modularity)
        plt.axvline(nclusters, c='r')
        plt.ylabel('modularity')
        plt.xlabel('number of clusters')

        outfile = f'{OUTDIR}/supercons_min{minK}max{maxK}_n{N_PER}' \
            + f'.cv{N_TOP_GENES}_k{n_neighbors}_{"sim_" if sim else ""}{WEIGHTING_TYPE}_{CLUSTER_TYPE}' \
            + f'{f"_s{STEPS}" if STEPS!=4 else ""}' \
            + f'.modularity_peak.pdf'
        print(outfile)
        plt.savefig(outfile, bbox_inches='tight', transparent=True)


        density_threshold = DENSITY_THRESHOLD # this means it is ignored

        eta_shp_med = eta_shp.median().values # vector of length total ngenes
        eta_rte_med = eta_rte.median().values
        beta_shp_med = beta_shp.groupby(cluster_labels).median() # n of walktrap clusters * total ngenes
        beta_rte_med = beta_rte.groupby(cluster_labels).median()

        eta_e_x_med = eta_shp_med/eta_rte_med
        beta_e_x_med = beta_shp_med/beta_rte_med

        outfile = outfile.replace('modularity_peak', 'clustergram')
        res = make_consensus_plot(factor_dists, cluster_labels, local_density, density_threshold=density_threshold)
        print(outfile)
        res[0].savefig(outfile, bbox_inches='tight', transparent=True)

        cluster_labels.value_counts()

        # removing small walktrap clusters
        min_cluster_size = max(2,MIN_COMMUNITY_SIZE) #int(local_neighborhood_size * len(my_models))
        print("Walktrap community size cutoff = "+ str(min_cluster_size))
        print("The smallest community size = "+ str(min(cluster_labels.value_counts())))
        cl_keep = np.where(cluster_labels.value_counts(sort=False) >= min_cluster_size)[0]
        print("# of communities kept = "+ str(len(cl_keep)))
        cluster_labels = cluster_labels.loc[cluster_labels.isin(cl_keep)]
        # len(cluster_labels.values)
        cluster_labels.value_counts


        MAX_PROJ = 1 # what for?

        outfile = f'{OUTDIR}/supercons_min{minK}max{maxK}_n{N_PER}' \
            + f'.cv{N_TOP_GENES}_k{n_neighbors}_{"sim_" if sim else ""}{WEIGHTING_TYPE}_{CLUSTER_TYPE}' \
            + f'{f"_s{STEPS}" if STEPS!=4 else ""}' \
            + f'.mcs{min_cluster_size}.proj_M{MAX_PROJ}.joblib'

        if os.path.exists(outfile):
            print(f'Loading from {outfile}')
            model = joblib.load(outfile)
        else:
            np.random.seed(0)
            nfactors = cluster_labels.nunique()
            print(nfactors)
            a = 0.3; c=0.3
            for m in my_models:
                if m.nfactors == nfactors:
                    a = m.a
                    c = m.c
                    break
            model = refit_local_params(X, (eta_shp_med, eta_rte_med, 
                                           beta_shp_med.iloc[cl_keep], 
                                           beta_rte_med.iloc[cl_keep]), 
                                   nfactors, 
                                   bp=my_models[0].bp, dp=my_models[0].dp,
                                   a=a, c=c,
                                   project_kw={'max_iter':MAX_PROJ})
            print(f'Saving to {outfile}')
            joblib.dump(model, outfile)
            
        training_loss = [(m.nfactors, m.loss[-1]) for m in models if m.nfactors==model.nfactors]
        
        np.random.seed(0)

        test_loss_eval = projection_loss_function(
            mean_negative_pois_llh, X_test, model.nfactors,
            #model_kwargs={'verbose':True},
            proj_kwargs={'reinit':True, 'verbose':False, 'max_iter':100, 'check_freq':10})

        orig_test_loss = []
        for m in models:
            if m.nfactors==model.nfactors:
                loss = test_loss_eval(a=m.a, ap=m.ap, bp=m.bp, c=m.c, cp=m.cp, dp=m.dp,
                                eta=m.eta, beta=m.beta)
                orig_test_loss.append(loss)
                print(m.nfactors, loss)

        print('\nMean test loss (random_init):', 
              np.mean(orig_test_loss), '+/-', np.std(orig_test_loss), sem(orig_test_loss))
        print( test_loss_eval(a=model.a, ap=model.ap, bp=model.bp, c=model.c, cp=model.cp, dp=model.dp,
                         eta=model.eta, beta=model.beta)
             )
        
        MAX_REFIT = 150

        test_loss = projection_loss_function(
            mean_negative_pois_llh, X_test, model.nfactors, 
            proj_kwargs={'reinit':False, 'verbose':False})

        outfile2 = outfile.replace('.joblib', f'.refit_M{MAX_REFIT}.joblib')

        if os.path.exists(outfile2):
            print(f'Loading from {outfile2}')
            model2 = joblib.load(outfile2)
        else:
            model2 = deepcopy(model)
            np.random.seed(0)
            model2.fit(X, loss_function=test_loss, reinit=False, verbose=True, max_iter=MAX_REFIT)
            print(f'Saving to {outfile2}')
            joblib.dump(model2, outfile2)
    
    
    elif args.cmd == 'score':
        print('Loading model.....')
        model = joblib.load(args.model)

        print('Calculating scores.....')
        cell_score = model.cell_score()
        gene_score = model.gene_score()

        print('Saving scores.....')
        np.savetxt(outprefix + 'cell_score.txt', cell_score, delimiter='\t')
        np.savetxt(outprefix + 'gene_score.txt', gene_score, delimiter='\t')

        print('Calculating mean cellscore fractions.....')
        frac_list = mean_cellscore_fraction_list(cell_score)
        with open(outprefix + 'mean_cellscore_fraction.txt', 'w') as h:
            h.write('nfactors\tmean_cellscore_fraction\n')
            for i,csf in enumerate(frac_list):
                h.write('{}\t{}\n'.format(i+1,csf))

        print('Calculating maximum pairwise overlaps.....')
        table = max_pairwise_table(gene_score, 
                ntop_list=[50,100,150,200,250,300,350,400,450,500])
        table.to_csv(outprefix + 'maximum_overlaps.txt', sep='\t', index=False)

        if args.genefile is not None:
            print('Ranking genes.....')
            # load and format gene file
            genes = np.loadtxt(args.genefile, delimiter='\t', dtype=str)
            if len(genes.shape) == 1:
                genes = genes[:,None]
            # get column to use for gene names
            last_col = genes.shape[1] - 1
            name_col = last_col if args.name_col > last_col else args.name_col
            print('.....using {}\'th column of genefile as gene label'.format(
                name_col))

            # rank the genes by gene_score
            ranks = np.argsort(gene_score, axis=0)[::-1]
            ranked_genes = []
            for i in range(gene_score.shape[1]):
                ranked_genes.append(genes[ranks[:,i], name_col])
            ranked_genes = np.stack(ranked_genes).T
            print('Saving ranked genes.....')
            np.savetxt(outprefix + 'ranked_genes.txt', ranked_genes, 
                    fmt="%s", delimiter='\t')

        print('Writing commandline arguments to file.....')
        cmdfile = '{}score_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2) 

    elif args.cmd == 'project':
        print('Loading reference model.....')
        model = joblib.load(args.model)
        print('Loading data.....')
        load_fnc = mmread if args.input.endswith('.mtx') else load_coo
        proj_data = load_fnc(args.input)
        print('Projecting data.....')
        projection = model.project(proj_data, replace=False, verbose=True,
                                   recalc_bp=args.recalc_bp,
                                   min_iter=args.min_iter, 
                                   max_iter=args.max_iter, 
                                   check_freq=args.check_freq, 
                                   epsilon=args.epsilon, )
        print('Saving projection.....')
        if args.recalc_bp:
            outprefix += '{}.'.format('recalc_bp')
        proj_out = '{}{}.proj.joblib'.format(outprefix, 
                args.model.rsplit('.',1)[0].split('/')[-1])
        joblib.dump(projection, proj_out)

        print('Writing commandline arguments to file.....')
        cmdfile = '{}project_commandline_args.json'.format(outprefix)
        with open(cmdfile, 'w') as f:
            json.dump(args.__dict__, f, indent=2)
